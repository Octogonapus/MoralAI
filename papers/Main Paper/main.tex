\documentclass[]{report}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{bera}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{placeins}

\graphicspath{{../../figures/}}

\title{
    {Moral AI IQP}\\
    {\large Worcester Polytechnic Institute}\\
    {\includegraphics[height=2in]{figures/WPI_Inst_Prim_FulClr.png}}
}
\author{Ryan Benasutti}
\date{April 2019}

\newcommand{\code}{\texttt}

\begin{document}

\maketitle

\clearpage
\mbox{}
\clearpage

\chapter*{Abstract}

Artificial intelligence is being deployed in increasingly autonomous systems where it will have to
make moral decisions. However, the rapid growth in artificial intelligence is outpacing the research
in building explainable systems. In this paper, a number of problems around one facet of explainable
artificial intelligence, training data, is explored. Possible solutions to these problems are
presented. Additionally, the human decision-making process in unavoidable accident scenarios is
explored through qualitative analysis of survey results.

\chapter*{Acknowledgements}

\begin{enumerate}
    \item Professor Therese Smith
    \item Professor Yunus Telliel
    \item Griffin Tabor
\end{enumerate}

\tableofcontents

\FloatBarrier
\chapter{Introduction}

This work has two primary goals. First, we seek to demonstrate how a neural network can learn a bias
and empirically determine the severity of that bias. Classification accuracy testing will be
employed to evaluate the trained neural network and determine if any bias was learned, and, if so,
the severity of that bias. Second, we also seek to understand the decision-making process in humans
behind making moral decisions in unavoidable accident scenarios, i.e., dilemmas. This part of the
research will be done by surveying a group of people and performing qualitative analysis on the
survey results. These results serve not only as a way to understand this decision-making process but
also as a language and structure we can use to craft communications with those unfamiliar with
artificial intelligence.

Chapter two provides an overview of what explainable AI is and the current demands for it, in
addition to covering prior research into how humans think when presented with a dilemma. Chapter
three discusses the architecture, training, and testing of a neural network and what questions we
asked in the aforementioned survey. Chapter four analyses the neural network testing results and the
survey results. Chapter five concludes this work and provides avenues for future inquiry.


\FloatBarrier
\chapter{Background}

Introduce background readings (maybe not first?).

Autonomous vehicle technology is growing rapidly and AI is a key piece of that technology. As this
technology gets closer to attaining full autonomy, the AI deployed in these systems will have
greater responsibility than ever. These AI systems must be explainably fair, i.e., they must both
make decisions using only the least amount of information necessary for optimal performance and make
those decisions predictably and correctly. For example, the AI in an autonomous vehicle does not
need to be supplied with information about a pedestrian's race, even though race may be an impactful
trait in other fields, especially medical fields~\cite{sickeCellDisease}. Furthermore, these AI
systems must also be explainable for legal reasons, such as determining which party is at fault in
the event of a car accident or, in the European Union, complying with a user's "right to
explanation"~\cite{goodman2017european}.

The demand for explainable AI is increasing, as illustrated by DARPA's Explainable Artificial
Intelligence (XAI) program~\cite{gunning2016explainable}. This program aims to develop explainable
AI systems such as in Figure~\ref{fig:darpa_xai}. There has also been a symposium focusing on AI
inclusivity towards marginalized peoples~\cite{berkmanKleinCenterAI2017,aiAndInclusionSymposium}.
This symposium illustrates the increasing need to discuss AI fairness and inclusivity in a way that
non-technical people can understand. One facet of this need that this paper addresses is the
question of specifically how much one needs to care about possible biases in the various stages of
AI architecture. Not all AI research involving morally responsible AI systems has a focus on
explainability, however. NVIDIA trained an end-to-end convolutional neural network, which "[maps]
raw pixels from a single front-facing camera directly to steering commands" \cite{bojarski2016end}.
With this approach, the AI will have to respond directly to pedestrians and other external stimuli.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1.1]{figures/xai-figure2.png}
    \caption[]{DARPA's XAI Concept~\protect\cite{gunningXAIProgram}}
    \label{fig:darpa_xai}
\end{figure}

Moving to the domain of how humans think about dilemmas, we look towards the Moral Machine
experiment~\cite{awad2018moral}, which is prior research into people's preferences in moral
dilemmas. This experiment involved an online survey in which participants are shown a moral dilemma
involving an autonomous vehicle, passengers, and pedestrians. In each dilemma, the participant must
choose between inaction, which typically results in the certain death of the pedestrians, and
action, which typically results in the certain death of the passengers. The study revealed three
strong global preferences towards sparing humans over animals, sparing more lives rather than fewer,
and sparing younger lives rather than older. The study also showed that some preferences vary
between countries depending on that country's propensity towards egalitarianism.

\FloatBarrier
\chapter{Methods}

\section{Data Generation}

The data is generated using a graphical model to control the conditional probabilities for the
states of each variable. The variables in the model correspond directly to the attributes of a
person. Figure~\ref{fig:graphical_model_image} is a rendering of the graphical model. For example,
people in the first option could be more likely to jaywalk than people in the second option,
producing a data set which is biased towards/against jaywalkers. When combined with control over the
number of people in each option, this method can produce both subtle and strong bias. The code for
the domain of each attribute of a person is in Figure~\ref{fig:code_for_person_attribute_domains}.
The Python library \href{https://github.com/pgmpy/pgmpy}{pgmpy} is used to create the graphical
model and infer each variable’s probability distribution. These distributions are then used to pick
elements from each variable’s domain. This process is repeated for each attribute of each person and
for the number of people in each option of a dilemma, forming a complete dilemma. The number of
dilemmas generated is specified programmatically using the \code{TrainMetadata} class, which
captures the number of dilemmas to generate and the maximum number of people per option.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{figures/network.png}
    \caption[]{The graphical model.}
    \label{fig:graphical_model_image}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{verbatim}
    age_states = [10, 20, 30, 40, 50, 60]
    race_states = [Race.white, Race.black, Race.asian,
                Race.native_american, Race.other_race]
    legal_sex_states = [LegalSex.male, LegalSex.female]
    jaywalking_states = [False, True]
    driving_under_the_influence_states = [False, True]
    \end{verbatim}
    \caption{Python code for the bracketed attributes of a Person}
    \label{fig:code_for_person_attribute_domains}
\end{figure}

\FloatBarrier
\section{Data Bracketing}
\label{sec:data-bracketing}

Attributes are one-hot encoded (i.e., mapped using an indicator function) so the neural network is
resilient to unspecified attributes. Age is bracketed by increments of 10 years. Some example
encoded ages are shown in Table~\ref{tab:example_age_attribute_encoding}. Boolean attributes are
encoded into three increments, as shown in Table~\ref{tab:example_boolean_attribute_encoding}.
    
\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c}
        Age (yr) & unspecified & 1-10 & 11-20 & 21-30 & 31-40 & 41-50 & 51-60 \\\hline
        unspecified & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
        3 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
        16 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
        42 & 0 & 0 & 0 & 0 & 0 & 1 & 0
    \end{tabular}
    \caption{Example age attribute encoding.}
    \label{tab:example_age_attribute_encoding}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c}
        Value & unspecified & false & true \\\hline
        unspecified & 1 & 0 & 0 \\
        false & 0 & 1 & 0 \\
        true & 0 & 0 & 1
    \end{tabular}
    \caption{Example boolean attribute encoding.}
    \label{tab:example_boolean_attribute_encoding}
\end{table}

\FloatBarrier
\section{Data Storage}

Data is stored using the JSON format using a serialization process called pickling via the Python
library \href{https://jsonpickle.github.io/}{jsonpickle}. This process was chosen because it
produces easily machine-readable files and because JSON is a popular data storage format. The
purpose of storing the generated data sets is to keep the data consistent between test iterations
and to share the data. Both the training and test data sets are pickled after generation.

\FloatBarrier
\section{Neural Network Model}

There are two primary requirements of the neural network used in the experiments. First, the network
must classify the training data. In other words, when given a dilemma, the network must classify
that dilemma by picking which option to avoid. For example, in a dilemma with two options of three
and four people, respectively, the correct classification is the second option because it allows the
autonomous vehicle to save more people. In the case where a dilemma has two or more options of equal
size, the earlier option is chosen.

Second, the network must be easy to train, meaning that the time required to train the network must
be small (on the order of minutes or less) and the hardware resources required to train the network
must be minor. Testing the network requires training it many times, so the time required to train
the network must be small. Additionally, the network will be trained on personal machines, so any
hardware requirements must be easy to meet.

There are three models which were considered when deciding on what network to use. First, an
autoencoder: autoencoders are trained using unsupervised learning, so labeling the data is not
necessary (want to avoid imparting a set of morals). This model would perform dimensionality
reduction, and perhaps learn to ignore noise (i.e., uniformly distributed attributes) in the data
set, but would be unable to classify the dilemmas.

Second, an autoencoder in combination with a simple neural network trained using supervised
learning: this model solves the classification problem which the previous model failed at, but
introduces unnecessary complexity to the research. The intent of this research is not to build a
neural network capable of guiding a real autonomous vehicle, so this model was deemed unnecessarily
complex.

Lastly, a recurrent neural network (RNN) with long short-term memory (LSTM). This option was
considered because RNN's are capable of accepting variable-length sequential data. We thought the
network may need to handle variable-length data, but the engineering challenge that design posed was
traded in favor of both limiting the maximum number of people in an option and bracketing the data
as covered in section~\ref{sec:data-bracketing}. Additionally, this network does not directly solve
the classification problem, so it is only marginally applicable for this research.

The final neural network chosen is a simple, shallow, feed-forward network with one hidden layer
trained using supervised learning, pictured in Figure~\ref{fig:shallow_neural_network}. The input
layer has dimensionality equal to the number of attributes per person (after one-hot encoding)
multiplied by the number of options per dilemma multiplied by the maximum number of people per
option. The output layer has dimensionality equal to the number of options per dilemma. The hidden
layer has dimensionality equal to the average of that of the input and output layers. An example
implementation in Keras of the model can be seen in Figure~\ref{fig:code_for_model}.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{figures/shallow_neural_network.png}
    \caption[]{A shallow feed-forward neural network with one hidden layer.}
    \label{fig:shallow_neural_network}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{verbatim}
    output_dim = 2
    input_dim = 22 * output_dim * \
            train_metadata.max_num_people_per_option

    model.add(Dense(units=input_dim, activation='relu',
                input_dim=input_dim))
    model.add(Dense(units=round((input_dim + output_dim) / 2),
                activation='relu'))
    model.add(Dense(units=output_dim, activation='softmax'))
    \end{verbatim}
    \caption{The Keras code for the neural network model.}
    \label{fig:code_for_model}
\end{figure}

\FloatBarrier
\section{Neural Network Training}

The training data given to the network is categorical, so the categorical cross entropy loss
function is used. As the network is quite simple, a stochastic gradient descent optimizer suffices.
Training happens over $5$ epochs with a batch size of $32$. An example implementation in Keras can
be seen in Figure~\ref{fig:code_for_training}; additionally, Figure~\ref{fig:neural_network_model}
provides a simple visualization of the dimensionality of the network.

\begin{figure}[h]
    \centering
    \begin{verbatim}
    model.compile(loss=losses.categorical_crossentropy,
                  optimizer='sgd',
                  metrics=[metrics.categorical_accuracy])
    
    model.fit(train_data, train_labels, epochs=5, batch_size=32)
    \end{verbatim}
    \caption{The Keras code for the neural network model.}
    \label{fig:code_for_training}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{figures/neural_network_model.png}
    \caption[]{The dimensions of the neural network used in this research.}
    \label{fig:neural_network_model}
\end{figure}

\FloatBarrier
\section{Neural Network Testing}

The neural network is tested using Keras to evaluate the classification accuracy and loss against a
test data set. The test data is generated using a graphical model in the same manner as the training
data; however, it is important to note that in this step, the test data is generated separately from
the training data: it is not a sampled subset of the training data. Each training data set is tested
five times. Each iteration involves training the neural network on the training data set and
evaluating its performance against a test data set to collect classification accuracy and loss
information. The results of all five iterations are averaged to produce an average classification
accuracy and loss. Many training data sets are generated and tested against the same test data set.
Each data point in Figure~\ref{fig:test_40-60_100-0_0-100_accuracy_plot} corresponds to a unique
training data set. All data points in the figure are tested against the same test data set.

The test result naming format, seen, for example, in the caption of
Figure~\ref{fig:test_40-60_100-0_0-100_accuracy_plot}, reproduced here in
Figure~\ref{fig:test_result_naming_example}, contains all the information necessary to understand
the characteristics of the test data set. The name can be split into three sections. First, "40-60"
refers to the probability distribution of people among the two options: $P(O_1) = 0.4$, $P(O_2) =
0.6$ (and for any dilemma with two options, $P(O_2) = P(\neg O_1)$). Second, "100-0" refers to the
probability distribution of jaywalking for those in the first option. $P(\neg J \mid O_1) = 1$, $P(J
\mid O_1) = 0$. Finally, "0-100" refers to the probability distribution of jaywalking for those in
the second option. $P(\neg J \mid O_2) = 0$, $P(J \mid O_2) = 1$.

\begin{figure}[h]
    \centering
    Classification accuracy against test 40-60 100-0 0-100
    \caption{The title of Figure~\ref{fig:test_40-60_100-0_0-100_accuracy_plot}}
    \label{fig:test_result_naming_example}
\end{figure}

\FloatBarrier
\section{Survey}

A group of primarily engineering-focused undergraduate college students was surveyed. These students
were in attendance of an introductory ethics course, so their responses are likely more ethically
sensitive than those of an average individual. The responses discussed later are still useful in
crafting communications with those outside of AI research, but might not extend as easily towards
the general public. Participation in the survey was voluntary and each question in the survey was
optional. The survey question are:
\begin{enumerate}
    \item If you had to choose between two accident scenarios, one in which a single person would
    certainly be killed, and the other which some number of people, a cluster of people, might be
    able to escape, how many people would that cluster have to contain (could be infinite), before
    you would choose the single person scenario?
    \item Please describe thoughts that went through your mind as you weighed the choice above.
    \item Would you trust AI to be race-blind if it were tested for not having racial bias?
    \item What would you need to know about the test, to feel protected against race-bias in AI?
\end{enumerate}

\FloatBarrier
\chapter{Findings and Analysis}

\section{Response to Bias}

Our research found that the AI became biased when the training data featured a strong trend not
present in the test data. For purposes of a control test, a test data set was generated with
uniformly distributed people and a uniformly distributed jaywalking probability.
Figure~\ref{fig:test_50-50_50-50_50-50_accuracy_plot} shows the result of this test. In this
scenario, $P(J)$ is observed to be independent of $P(O_1)$. When $P(J \mid O_1) < 0.1$, the neural
network classifies dilemmas incorrectly. This trend continues as $P(J \mid O_1)$ decreases, thereby
increasing the severity of the bias in the training data set. A similar but opposite trend occurs
when $P(J \mid O_1) > 0.9$.

Observing the contour plot in Figure~\ref{fig:test_40-60_100-0_0-100_accuracy_plot}, one can see
that classification accuracy decreases abnormally (i.e., differently than in the control in
Figure~\ref{fig:test_50-50_50-50_50-50_accuracy_plot}) when $P(O_1) > 0.6$ and $P(J \mid O_1) <
0.2$. In this area, the training data set consists mostly of people in the first option. Most people
in the first option are not jaywalkers and most people in the second option are. Therefore, the
training data set is biased to prefer non-jaywalkers because they appear disproportionately
frequently in the (larger) first option. The neural network, now having learned this trend, is
tested against a test data set in which most people are in the second option. Those in the first
option are not jaywalkers and those in the second option are. The network tends to select the first
option because it contains far fewer jaywalkers than the second option, despite the first option
being smaller than the second and therefore the incorrect choice. This causes the network's
classification accuracy to decrease in this region. The trend continues as $P(O_1)$ increases while
$P(J \mid O_1)$ decreases, thereby increasing the severity of the bias in the training data set.
Another view into the network's decisions is Figure~\ref{fig:test_40-60_100-0_0-100_jay_prob_plot},
which shows the real value of $P(J)$ when the network classified a dilemma incorrectly. In the areas
of the contour plot corresponding to Figure~\ref{fig:test_40-60_100-0_0-100_accuracy_plot}'s areas
of worst accuracy, we can see that the real value of $P(J)$ is close to zero. In other words, the
network performs worst when it picks an option because that option is absent of jaywalkers (i.e.,
when the network makes a decision based on the bias it learned rather than the rule used to label
the test data set). This is further evidence that the network has learned a bias against jaywalkers.
A similar but opposite trend occurs when $P(O_1) < 0.4$ and $P(J \mid O_1) > 0.8$.

\FloatBarrier
\section{Avoiding Bias}

Three ways for the neural network to learn a bias
\begin{enumerate}
    \item Flaws in data
    \begin{enumerate}
        \item Any information which is not strictly necessary for the neural network to make
        effective decisions should not be given to the network. This especially affects end-to-end
        networks such as the convolutional neural network in~\cite{bojarski2016end} because these
        systems have an enormous variety of (sometimes) unfiltered data given to them. This approach
        can increase the risk of the neural network learning a bias.
    \end{enumerate}
    
    \item Flaws in AI architecture
    \begin{enumerate}
        \item This research uses a simple, shallow neural network to reduce architectural
        complexity. Deeper networks, specifically deep convolutional networks, undoubtedly perform
        better, but these network architectures suffer from increased design complexity and
        increased training difficulty~\cite{mhaskar2016deep}.

        \item Neural networks used for image-based object detection suffer from predictive
        inequality in detecting people of differing skin tones~\cite{wilson2019predictive}. Those
        with skin tones in the Fitzpatrick range $[1, 3]$ are more accurately identified than those
        with skin tones in the $[4, 6]$ range. Furthermore, this problem persists between networks
        of different architectures. Although the authors of that research propose a different loss
        function which decreases predictive inequality, one could imagine a totally different system
        which does not use color cameras at all. Infrared cameras may serve as a good replacement
        because they produce images which are easier to filter with traditional computer vision
        techniques than images from color cameras are.
    \end{enumerate}
    
    \item Flaws in testing
    \begin{enumerate}
        \item If one is concerned that a system may be less able to measure some attribute in a
        certain environment, then the testing for the system may want to overrepresent that
        attribute. In the context of~\cite{wilson2019predictive}, the test data set used might
        consist of a majority of images of people with skin tones in the Fitzpatrick range $[4, 6]$,
        regardless of whether the system is expected to operate in an environment consistent with
        that skin tone distribution or not.

        \item If a system should be equally sensitive to all of its inputs, then those input should
        be represented equally during testing, even if a different distribution of inputs is
        expected to be encountered when the system is deployed. In the case of autonomous vehicles,
        this research assumed the neural network should treat all people equally, but in reality
        this is a region-specific measure. The Moral Machine experiment measured strong regional
        preferences for various aspects of decision-making during dilemmas~\cite{awad2018moral}. For
        example, Eastern countries showed an almost nonexistent preference for sparing the young
        compared to Western and Southern countries. Southern countries showed a strong preference
        for sparing females compared to Western and Eastern countries. Not all regional preferences
        can be reliably accounted for, however. Some preferences, such as the Eastern countries
        stronger preference for sparing the lawful compared to Western and Southern countries, is
        not entirely enforceable by AI systems. Some instances of lawfulness classification could be
        reliable, such as detecting jaywalkers, but others, such as detecting unlawful intoxication,
        are most likely difficult. There is, of course, the question of whether or not autonomous
        vehicles should contain any regional preferences or whether they should be totally fair;
        however, that is outside the scope of this paper.
    \end{enumerate}
\end{enumerate}
    
\FloatBarrier
\section{Survey Results}

Thirty-five survey responses were collected. We found several themes in these responses, which are
summarized in Figure~\ref{fig:survey_analysis} and explained in greater detail here:
\begin{enumerate}
    \item Humans can be flawed
    \begin{itemize}
        \item Humans can have bias.
        \item Flaws in humans can lead to flaws in AI systems design and in data sets.
    \end{itemize}
    
    \item Data can be flawed
    \begin{itemize}
        \item AI systems trained on flawed data will show flawed performance.
    \end{itemize}
    
    \item Value saving more lives over fewer
    \begin{itemize}
        \item This is consistent with one of the Moral Machine experiment's
        findings~\cite{awad2018moral}.
    \end{itemize}
    
    \item Unwillingness to kill
    \begin{itemize}
        \item Killing is generally unjustifiable; therefore, an action that causes a greater number
        of people to be spared is not necessarily desirable.
        \item If there is an option with a chance that people might not die, that option is more
        desirable than an option containing one person who will certainly be killed, despite the
        prior option containing more people.
    \end{itemize}
    
    \item Demand for testing
    \begin{itemize}
        \item Because both humans and data can be flawed, testing must be employed to validate the
        fairness of the AI system.
        \item These tests must have great breadth and cover many scenarios and people, especially
        minorities.
        \item The data used for training and testing should be transparent.
        \item The methods of testing should be transparent.
        \item Good testing methodologies and good results can lead to trust in AI systems.
    \end{itemize}
\end{enumerate}

How to address these concerns
\begin{itemize}
    \item People can be flawed
    \begin{itemize}
        \item This can be addressed with architectural approaches to AI system design. We can
        restrict what features the neural network has access to. We can evaluate different sensor
        technologies and select the sensor that performs best in the system's expected operating
        environment. We can employ low-level safety mechanisms to catch some erroneous decisions the
        neural network may make.
        \item Should I explain the restriction layer in more detail here? Perhaps link to a
        publication, if there is one?
    \end{itemize}
    
    \item Data can be flawed
    \begin{itemize}
        \item This can be addressed with sensitivity studies. This research provides a format to
        determine an neural network's sensitivity to flawed data.
    \end{itemize}

    \item People have varying opinions of how an autonomous vehicle should react in dilemma
    situations.
    \begin{itemize}
        \item An AI system operating an autonomous vehicle is in control at all times. The system
        must decide what to do in a dilemma. There is no inaction: choosing inaction is equivalent
        to choosing an action with the same result as inaction. Therefore, the system must be
        programmed, either explicitly through traditional techniques or implicitly through machine
        learning, to respond a specific way in a dilemma. Not all end-users may find the system's
        programming ethically acceptable.

        \item Some survey respondents expressed interest in a "personal AI" solution in which the
        end-user of an autonomous vehicle can input their preferences to modify how the AI will
        react in a dilemma.
        
        \item A personal AI system brings with it a number of ethical questions
        \begin{itemize}
            \item What parameters should be tunable? Certainly the end-user should not be able to
            tell the AI to prefer one race over another.
            \item Should the set of tunable parameters be regulated such that all autonomous
            vehicles are required to have the same set of tunable parameters?
            \item How liable is the autonomous vehicle manufacturer and any other parties involved
            in training/testing the AI system?
            \item How liable is the end-user in tuning their autonomous vehicle (perhaps a tuning
            decision they made caused greater personal injury than if they had chosen a different
            option).
        \end{itemize}
    \end{itemize}
\end{itemize}

\FloatBarrier
\chapter{Conclusion and Future Work}
\section{Conclusion}

\begin{enumerate}
    \item Our research found that AI becomes biased when ?. Therefore, one must take a certain
    amount of care in dealing with bias in data (how much?).
    
    \item In order to avoid training biased AI, we recommend formatting training data such that ?.
    
    \item We also recommend that
    \begin{itemize}
        \item Teams that work with AI, especially teams which create or train AI, should include
        social scientists.
        
        \item AI could be verified by 3rd party groups in addition to a team's internal testing.
    \end{itemize}
\end{enumerate}

\FloatBarrier
\section{Future Work}

\begin{itemize}
    \item Explanations, whether of AI decisions, architecture, or other, must be delivered in a way
    that the user can understand. As Gilpin puts it, "The success of this goal is tied to the
    cognition, knowledge and biases of the user: for a system to be interpretable, it must produce
    descriptions that are simple enough for a person to understand using a vocabulary that is
    meaningful to the user"~\cite{gilpin2018explaining}. This paper has attempted to show
    specifically how much care one must take in dealing with bias in data, but more attention is
    needed in other areas of AI systems architecture.

    \item Repeat this work's sensitivity testing on current state-of-the-art models and determine
    their sensitivity to bias in training data.

    \item Develop a software library to report correlations in data. This library would accept a
    data set, analyse it to find any and all correlations between different attributes, and report
    those correlations and their associated strengths. The intent is to use this software to
    proactively detect possible false correlations in data sets before they are used for training or
    testing.
\end{itemize}

\bibliography{citations}
\bibliographystyle{plain}

\appendix
\chapter{Figures}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.46]{figures/moral_ai_survey_analysis.jpg}
    \caption[]{Qualitative analysis of the survey results.}
    \label{fig:survey_analysis}
\end{figure}

% 
% test 40-60 100-0 0-100
% 

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_100-0_0-100_accuracy.png}}
    \caption[]{The classification accuracy against \code{test 40-60 100-0 0-100}.}
    \label{fig:test_40-60_100-0_0-100_accuracy_plot}
\end{figure}

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_100-0_0-100_loss.png}}
    \caption[]{The loss against \code{test 40-60 100-0 0-100}.}
    \label{fig:test_40-60_100-0_0-100_loss_plot}
\end{figure}

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_100-0_0-100_jay_prob.png}}
    \caption[]{The actual jaywalking probability when classified incorrectly against \code{test 40-60 100-0 0-100}.}
    \label{fig:test_40-60_100-0_0-100_jay_prob_plot}
\end{figure}

% 
% test 40-60 0-100 100-0
% 

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_0-100_100-0_accuracy.png}}
    \caption[]{The classification accuracy against \code{test 40-60 0-100 100-0}.}
    \label{fig:test_40-60_0-100_100-0_accuracy_plot}
\end{figure}

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_0-100_100-0_loss.png}}
    \caption[]{The loss against \code{test 40-60 0-100 100-0}.}
    \label{fig:test_40-60_0-100_100-0_loss_plot}
\end{figure}

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_0-100_100-0_jay_prob.png}}
    \caption[]{The actual jaywalking probability when classified incorrectly against \code{test 40-60 0-100 100-0}.}
    \label{fig:test_40-60_0-100_100-0_jay_prob_plot}
\end{figure}

% 
% test 40-60 80-20 20-80
% 

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_80-20_20-80_accuracy.png}}
    \caption[]{The classification accuracy against \code{test 40-60 80-20 20-80}.}
    \label{fig:test_40-60_80-20_20-80_accuracy_plot}
\end{figure}

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_80-20_20-80_loss.png}}
    \caption[]{The loss against \code{test 40-60 80-20 20-80}.}
    \label{fig:test_40-60_80-20_20-80_loss_plot}
\end{figure}

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_80-20_20-80_jay_prob.png}}
    \caption[]{The actual jaywalking probability when classified incorrectly against \code{test 40-60 80-20 20-80}.}
    \label{fig:test_40-60_80-20_20-80_jay_prob_plot}
\end{figure}

% 
% test 40-60 20-80 80-20
% 

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_20-80_80-20_accuracy.png}}
    \caption[]{The classification accuracy against \code{test 40-60 20-80 80-20}.}
    \label{fig:test_40-60_20-80_80-20_accuracy_plot}
\end{figure}

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_20-80_80-20_loss.png}}
    \caption[]{The loss against \code{test 40-60 20-80 80-20}.}
    \label{fig:test_40-60_20-80_80-20_loss_plot}
\end{figure}

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_40-60_20-80_80-20_jay_prob.png}}
    \caption[]{The actual jaywalking probability when classified incorrectly against \code{test 40-60 20-80 80-20}.}
    \label{fig:test_40-60_20-80_80-20_jay_prob_plot}
\end{figure}

% 
% test 50-50 50-50 50-50
% 

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_50-50_50-50_50-50_accuracy.png}}
    \caption[]{The classification accuracy against \code{test 50-50 50-50 50-50}.}
    \label{fig:test_50-50_50-50_50-50_accuracy_plot}
\end{figure}

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_50-50_50-50_50-50_loss.png}}
    \caption[]{The loss against \code{test 50-50 50-50 50-50}.}
    \label{fig:test_50-50_50-50_50-50_loss_plot}
\end{figure}

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[scale=0.55]{test_50-50_50-50_50-50_jay_prob.png}}
    \caption[]{The actual jaywalking probability when classified incorrectly against \code{test 50-50 50-50 50-50}.}
    \label{fig:test_50-50_50-50_50-50_jay_prob_plot}
\end{figure}

\end{document}
